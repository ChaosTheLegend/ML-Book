{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Attention mechanism\n",
    "\n",
    "In a basic RNN, each recurrent neuron receives inputs from all neurons from the previous time step, as well as the inputs from the current time step, hence the term 'recurrent'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ffcb30cf3fce291"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66e05d8454aad315"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.src.layers import SimpleRNN, Dense\n",
    "from keras import Sequential\n",
    "\n",
    "input_dim = 10  # change this to the number of input features in your dataset\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, activation='relu', input_shape=(None, input_dim)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, epochs=100, verbose=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32987f6160ec0fcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# make predictions and calculate accuracy\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "842d99705f3300cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alignment Scores in RNN Attention Mechanism\n",
    "\n",
    "The attention mechanism in a recurrent neural network (RNN) uses alignment scores to determine how much focus to place on each input in a sequence.\n",
    "\n",
    "In the context of sequence-to-sequence models, for example, if we have an encoded input sequence, an alignment score is computed for each pair of input and output positions. If \"a\" is the decoder’s hidden state and \"b\" is all of the encoder’s hidden states, the alignment score function often takes the following form:\n",
    "\n",
    "$$\n",
    "\\text{score}(a, b) = a^Tb\n",
    "$$\n",
    "\n",
    "This score indicates how well the inputs around position \"b\" and the output at position \"a\" match. The alignment scores for each input \"b\" are combined into a single vector and normalized to sum to 1, resulting in the attention weights. These weights determine the amount of 'attention' given by the model to each input timestep while producing an output. \n",
    "\n",
    "Higher alignment scores mean that the decoder pays more attention to those parts of the encoder's output.\n",
    "\n",
    "Overall, the attention mechanism improves the accuracy of the RNN model when handling tasks with long input sequences, by enabling it to focus on the most relevant parts of the input to produce a given output."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e761261244e15d12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## From Alignment Scores to Attention Weights\n",
    "\n",
    "After computing the alignment scores between the input and output vectors in the attention mechanism, these scores are then converted to attention weights using the softmax function.\n",
    "\n",
    "The softmax function is commonly used in neural networks to turn scores into probabilities. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "By applying softmax, we ensure that each weight falls in the range [0, 1] and that they all sum to 1, which allows us to interpret the attention weights as probabilities.\n",
    "\n",
    "In the context of attention mechanisms, the softmax function is applied to the alignment scores for each input and output pair, resulting in attention weights. Therefore, each input element in a sequence gets an attention weight. Now the model knows how much attention it needs to pay to each element when encoding information.\n",
    "\n",
    "For a given output, positions in the input sequence with a higher attention weight have a greater influence on the computation. This means our decoder will \"pay more attention\" to these positions during the encoding of the sequence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f59370463267f6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
