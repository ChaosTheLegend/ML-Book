{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Attention mechanism\n",
    "\n",
    "In a basic RNN, each recurrent neuron receives inputs from all neurons from the previous time step, as well as the inputs from the current time step, hence the term 'recurrent'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ffcb30cf3fce291"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66e05d8454aad315"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.src.layers import SimpleRNN, Dense\n",
    "from keras import Sequential\n",
    "\n",
    "input_dim = 10  # change this to the number of input features in your dataset\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, activation='relu', input_shape=(None, input_dim)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, epochs=100, verbose=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32987f6160ec0fcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# make predictions and calculate accuracy\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "842d99705f3300cb"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjupyterquiz\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m display_quiz\n\u001B[0;32m      3\u001B[0m git_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://raw.githubusercontent.com/ChaosTheLegend/ML-Book/Quizes/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 6\u001B[0m display_quiz(git_path\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquiz1.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\jupyterquiz\\dynamic.py:169\u001B[0m, in \u001B[0;36mdisplay_quiz\u001B[1;34m(ref, num, shuffle_questions, shuffle_answers, preserve_responses, border_radius, question_alignment, max_width, colors)\u001B[0m\n\u001B[0;32m    167\u001B[0m     script\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39mtext\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 169\u001B[0m     file \u001B[38;5;241m=\u001B[39m urllib\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39murlopen(url)\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m file:\n\u001B[0;32m    171\u001B[0m         script \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m line\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\urllib\\request.py:216\u001B[0m, in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    215\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[1;32m--> 216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m opener\u001B[38;5;241m.\u001B[39mopen(url, data, timeout)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\urllib\\request.py:525\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m processor \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_response\u001B[38;5;241m.\u001B[39mget(protocol, []):\n\u001B[0;32m    524\u001B[0m     meth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(processor, meth_name)\n\u001B[1;32m--> 525\u001B[0m     response \u001B[38;5;241m=\u001B[39m meth(req, response)\n\u001B[0;32m    527\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\urllib\\request.py:634\u001B[0m, in \u001B[0;36mHTTPErrorProcessor.http_response\u001B[1;34m(self, request, response)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001B[39;00m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;66;03m# request was successfully received, understood, and accepted.\u001B[39;00m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m code \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m):\n\u001B[1;32m--> 634\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparent\u001B[38;5;241m.\u001B[39merror(\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp\u001B[39m\u001B[38;5;124m'\u001B[39m, request, response, code, msg, hdrs)\n\u001B[0;32m    637\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\urllib\\request.py:563\u001B[0m, in \u001B[0;36mOpenerDirector.error\u001B[1;34m(self, proto, *args)\u001B[0m\n\u001B[0;32m    561\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_err:\n\u001B[0;32m    562\u001B[0m     args \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mdict\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp_error_default\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m orig_args\n\u001B[1;32m--> 563\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_chain(\u001B[38;5;241m*\u001B[39margs)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\urllib\\request.py:496\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    494\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[0;32m    495\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[1;32m--> 496\u001B[0m     result \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m    497\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    498\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\urllib\\request.py:643\u001B[0m, in \u001B[0;36mHTTPDefaultErrorHandler.http_error_default\u001B[1;34m(self, req, fp, code, msg, hdrs)\u001B[0m\n\u001B[0;32m    642\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttp_error_default\u001B[39m(\u001B[38;5;28mself\u001B[39m, req, fp, code, msg, hdrs):\n\u001B[1;32m--> 643\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(req\u001B[38;5;241m.\u001B[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001B[1;31mHTTPError\u001B[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "\n",
    "git_path=\"https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/Quizes/\"\n",
    "\n",
    "\n",
    "display_quiz(git_path+\"quiz1.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T18:37:10.855459900Z",
     "start_time": "2023-12-11T18:37:09.405918900Z"
    }
   },
   "id": "21157c34b270034e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alignment Scores in RNN Attention Mechanism\n",
    "\n",
    "The attention mechanism in a recurrent neural network (RNN) uses alignment scores to determine how much focus to place on each input in a sequence.\n",
    "\n",
    "In the context of sequence-to-sequence models, for example, if we have an encoded input sequence, an alignment score is computed for each pair of input and output positions. If \"a\" is the decoder’s hidden state and \"b\" is all of the encoder’s hidden states, the alignment score function often takes the following form:\n",
    "\n",
    "$$\n",
    "\\text{score}(a, b) = a^Tb\n",
    "$$\n",
    "\n",
    "This score indicates how well the inputs around position \"b\" and the output at position \"a\" match. The alignment scores for each input \"b\" are combined into a single vector and normalized to sum to 1, resulting in the attention weights. These weights determine the amount of 'attention' given by the model to each input timestep while producing an output. \n",
    "\n",
    "Higher alignment scores mean that the decoder pays more attention to those parts of the encoder's output.\n",
    "\n",
    "Overall, the attention mechanism improves the accuracy of the RNN model when handling tasks with long input sequences, by enabling it to focus on the most relevant parts of the input to produce a given output."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e761261244e15d12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## From Alignment Scores to Attention Weights\n",
    "\n",
    "After computing the alignment scores between the input and output vectors in the attention mechanism, these scores are then converted to attention weights using the softmax function.\n",
    "\n",
    "The softmax function is commonly used in neural networks to turn scores into probabilities. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "By applying softmax, we ensure that each weight falls in the range [0, 1] and that they all sum to 1, which allows us to interpret the attention weights as probabilities.\n",
    "\n",
    "In the context of attention mechanisms, the softmax function is applied to the alignment scores for each input and output pair, resulting in attention weights. Therefore, each input element in a sequence gets an attention weight. Now the model knows how much attention it needs to pay to each element when encoding information.\n",
    "\n",
    "For a given output, positions in the input sequence with a higher attention weight have a greater influence on the computation. This means our decoder will \"pay more attention\" to these positions during the encoding of the sequence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f59370463267f6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
