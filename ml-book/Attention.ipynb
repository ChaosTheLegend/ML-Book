{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb71ccca406f598",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffcb30cf3fce291",
   "metadata": {},
   "source": [
    "## RNN recap\n",
    "\n",
    "In a basic RNN, each recurrent neuron receives inputs from all neurons from the previous time step, as well as the inputs from the current time step, hence the term 'recurrent'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e05d8454aad315",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "ExecuteTime": {
     "end_time": "2023-12-13T03:44:14.442287200Z",
     "start_time": "2023-12-13T03:44:09.315748300Z"
    }
   },
   "outputs": [],
   "source": [
    "### This cell should be hidden in the final version\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.src.utils import pad_sequences\n",
    "from jupyterquiz import display_quiz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "git_path=\"https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Quizes/\"\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "max_len = 200\n",
    "x_train = pad_sequences(x_train, maxlen=max_len, truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_len, truncating='post')\n",
    "num_words = 10000\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "dropout_rate = 0.5\n",
    "\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#    tf.keras.layers.Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_len),\n",
    "#    tf.keras.layers.SimpleRNN(hidden_dim),\n",
    "#    tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model is loaded from the file to save time\n",
    "\n",
    "simpleRNN = tf.keras.models.load_model('simpleRNN.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make predictions and calculate accuracy\n",
    "\n",
    "y_pred = simpleRNN.predict(x_test)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "simple_accuracy = accuracy_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "221e9404183625dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_epochs = pd.read_csv('simplernn_accuracy.csv')\n",
    "\n",
    "plt.plot(accuracy_epochs['epoch'], accuracy_epochs['accuracy'])\n",
    "\n",
    "plt.title('Accuracy of Simple RNN')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# add final accuracy to the plot\n",
    "\n",
    "plt.text(17, 0.52, 'Final Accuracy: ' + str(round(simple_accuracy, 2)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d5ea1432578c4b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Need for Attention Mechanism\n",
    "\n",
    "The problem with basic RNNs is that they are not very good at handling long sequences. \n",
    "\n",
    "Even when using more epochs, the accuracy of the model does not improve much. This is because the model is not able to learn the long-term dependencies in the data.\n",
    "\n",
    "This is known as the vanishing gradient problem."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56a4f85fa9b69ba4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vanishing Gradient Problem\n",
    "\n",
    "The vanishing gradient problem occurs when the gradients of the loss function become increasingly smaller as the model learns to associate inputs and outputs that are further apart in time.\n",
    "\n",
    "This leads to the model \"forgetting\" the information from the earlier inputs, which makes it difficult to learn long-term dependencies.\n",
    "\n",
    "![Simple RNN](https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Images/SimpleRNN.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "739f71659e8581c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Math Behind Vanishing Gradient Problem\n",
    "\n",
    "The vanishing gradient problem occurs because of the way gradients are computed in RNNs:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial h} \\frac{\\partial h}{\\partial W}\n",
    "$$\n",
    "\n",
    "The gradient is computed by multiplying the gradients of the loss function with respect to the output, the output with respect to the hidden state, and the hidden state with respect to the weights.\n",
    "\n",
    "Since gradients are multiplied together, if the gradients at each time step are less than 1 (e.g., due to using activation functions like sigmoid or tanh), this multiplication leads to a compounding effect. As you go further back in time, the gradients become increasingly smaller.\n",
    "\n",
    "\n",
    "![Simple RNN](https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Images/SimpleRNNProblem.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "867b4fd131cd7026"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "To combat the vanishing gradient problem, we can use an attention mechanism.\n",
    "\n",
    "An attention mechanism is a way to help RNNs learn long-term dependencies by allowing the model to focus on the most relevant parts of the input sequence when producing a given output.\n",
    "\n",
    "We do this by adding a context vector to the model, which is a weighted sum of the encoder's hidden states. The weights are computed using an alignment score function, which measures how well the inputs around a given position and the output at that position match.\n",
    "\n",
    "![Attention Mechanism](https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Images/Attention.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3be23744b7a0443"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Attention, Bidirectional, Dropout, SimpleRNN\n",
    "import os\n",
    "\n",
    "inputs = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_len)(inputs)\n",
    "rnn = SimpleRNN(hidden_dim, return_sequences=True)(embedding)\n",
    "attention = Attention()([rnn, rnn])\n",
    "context = tf.reduce_sum(attention * rnn, axis=1)\n",
    "outputs = Dense(output_dim, activation='sigmoid')(context)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fdbf3407cb4f725"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "model = tf.keras.models.load_model('simpleRNN_attention.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f5b5853be0026b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Math Behind Attention Mechanism\n",
    "\n",
    "The context vector is computed as follows:\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j\n",
    "$$\n",
    "\n",
    "where $c_i$ is the context vector at position $i$, $T_x$ is the length of the input sequence, $\\alpha_{ij}$ is the alignment score between the output at position $i$ and the input at position $j$, and $h_j$ is the hidden state at position $j$.\n",
    "\n",
    "![Attention Mechanism](https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Images/AttentionLive.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd471edc7084dba"
  },
  {
   "cell_type": "raw",
   "source": [
    "In the end we get a matrix of context vectors, which tell us which parts of the input sequence are most relevant to output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7b7c8fb6d10991e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "attention_layer = model.layers[2]\n",
    "\n",
    "\n",
    "weights = attention_layer.get_weights()[1]\n",
    "\n",
    "# sum all the columns in the weights matrix\n",
    "\n",
    "weights = np.sum(weights, axis=1)\n",
    "\n",
    "weights = weights[:20]\n",
    "\n",
    "plt.bar(range(weights.shape[0]), weights)\n",
    "\n",
    "plt.title('Attention Weights')\n",
    "\n",
    "plt.xlabel('Embedded word index')\n",
    "\n",
    "plt.ylabel('Word Weight')\n",
    "\n",
    "# make x axis use whole numbers\n",
    "\n",
    "plt.xticks(range(weights.shape[0]), range(weights.shape[0]))\n",
    "\n",
    "# color all negative weight bars red and positive weights blue\n",
    "\n",
    "for i in range(weights.shape[0]):\n",
    "    if weights[i] < 0:\n",
    "        plt.gca().get_children()[i].set_color('red')\n",
    "    else:\n",
    "        plt.gca().get_children()[i].set_color('green')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24c15ec0bc49904c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Lower the absolute value of the weight, the less relevant the word is to the output\n",
    "\n",
    "Green bars show positive weights, indicating that the word is positive\n",
    "\n",
    "Red bars show negative weights, indicating that the word is negative"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c39c110a2d8583ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "attention_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "attention_accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8eb7ab4ec8f1557a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "By adding an attention mechanism, our model performs way better even when using the low number of epochs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5458b17b923489b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# draw a bar chart to compare the accuracy of the two models\n",
    "\n",
    "accuracy = [attention_accuracy, simple_accuracy];\n",
    "\n",
    "\n",
    "plt.bar(['Attention (5 epochs)', 'Simple RNN (50 epochs)'], accuracy)\n",
    "\n",
    "# add a title to the plot\n",
    "\n",
    "plt.title('Accuracy of Simple RNN vs Attention Mechanism')\n",
    "\n",
    "# add a label to the y-axis\n",
    "\n",
    "plt.ylabel('Accuracy')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12437d27e0a09305"
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
