{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb71ccca406f598",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffcb30cf3fce291",
   "metadata": {},
   "source": [
    "## RNN recap\n",
    "\n",
    "In a basic RNN, each recurrent neuron receives inputs from all neurons from the previous time step, as well as the inputs from the current time step, hence the term 'recurrent'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e05d8454aad315",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "### This cell should be hidden in the final version\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.src.utils import pad_sequences\n",
    "from jupyterquiz import display_quiz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "git_path=\"https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Quizes/\"\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "max_len = 200\n",
    "x_train = pad_sequences(x_train, maxlen=max_len, truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_len, truncating='post')\n",
    "num_words = 10000\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "dropout_rate = 0.5\n",
    "\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#    tf.keras.layers.Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_len),\n",
    "#    tf.keras.layers.SimpleRNN(hidden_dim),\n",
    "#    tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model is loaded from the file to save time\n",
    "\n",
    "simpleRNN = tf.keras.models.load_model('simpleRNN.keras')\n",
    "\n",
    "# make predictions and calculate accuracy\n",
    "\n",
    "y_pred = simpleRNN.predict(x_test)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "simple_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy_epochs = pd.read_csv('simplernn_accuracy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95b4309cb5f308",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(git_path + 'quiz_rnn.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88afc4c6da9fcbd",
   "metadata": {},
   "source": [
    "When applying RNNs to IMDB reviews, we get the following accuracy graph over 50 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13bad8c399f629",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracy_epochs['epoch'], accuracy_epochs['accuracy'])\n",
    "\n",
    "plt.title('Accuracy of Simple RNN')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# add final accuracy to the plot\n",
    "\n",
    "plt.text(17, 0.52, 'Final Accuracy: ' + str(round(simple_accuracy, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52007ce4221a2",
   "metadata": {},
   "source": [
    "However, we can see that the accuracy of the model does not improve much after the first few epochs. This forces us to use a large number of epochs to get a good accuracy, which is not ideal, and even then, the accuracy may not improve by much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90419af88e83fc",
   "metadata": {},
   "source": [
    "## The Need for Attention Mechanism\n",
    "::::{important}\n",
    "The problem with basic RNNs is that they are not very good at handling long sequences. \n",
    "\n",
    "Even when using more epochs, the accuracy of the model does not improve much. This is because the model is not able to learn the long-term dependencies in the data.\n",
    "\n",
    "This is known as the vanishing gradient problem.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fabaa35bffd3e95",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem\n",
    "\n",
    "The vanishing gradient problem occurs when the gradients of the loss function become increasingly smaller as the model learns to associate inputs and outputs that are further apart in time.\n",
    "\n",
    "This leads to the model \"forgetting\" the information from the earlier inputs, which makes it difficult to learn long-term dependencies.\n",
    "\n",
    "![Simple RNN](https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Images/SimpleRNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966d89f01fb6e0d",
   "metadata": {},
   "source": [
    "### Math Behind Vanishing Gradient Problem\n",
    "\n",
    "The vanishing gradient problem occurs because of the way gradients are computed in RNNs:\n",
    "\n",
    "::::{note}\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial h} \\frac{\\partial h}{\\partial W}\n",
    "$$\n",
    "::::\n",
    "\n",
    "The gradient is computed by multiplying the gradients of the loss function with respect to the output, the output with respect to the hidden state, and the hidden state with respect to the weights.\n",
    "\n",
    "::::{important}\n",
    "Since gradients are multiplied together, if the gradients at each time step are less than 1 (e.g., due to using activation functions like sigmoid or tanh), this multiplication leads to a compounding effect. As you go further back in time, the gradients become increasingly smaller.\n",
    "::::\n",
    "\n",
    "![Simple RNN](https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Images/SimpleRNNProblem.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62507b88f36efd2c",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(git_path + 'quiz_vanish.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33edbcad9a58f381",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "To combat the vanishing gradient problem, we can use an attention mechanism.\n",
    "\n",
    "::::{important}\n",
    "An attention mechanism is a way to help RNNs learn long-term dependencies by allowing the model to focus on the most relevant parts of the input sequence when producing a given output.\n",
    "::::\n",
    "\n",
    "We do this by adding a context vector to the model, which is a weighted sum of the encoder's hidden states. The weights are computed using an alignment score function, which measures how well the inputs around a given position and the output at that position match.\n",
    "\n",
    "![Attention Mechanism](https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Images/Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01e4a44e2de4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Attention, Bidirectional, Dropout, SimpleRNN\n",
    "import os\n",
    "\n",
    "inputs = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_len)(inputs)\n",
    "rnn = SimpleRNN(hidden_dim, return_sequences=True)(embedding)\n",
    "attention = Attention()([rnn, rnn])\n",
    "context = tf.reduce_sum(attention * rnn, axis=1)\n",
    "outputs = Dense(output_dim, activation='sigmoid')(context)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# load the model\n",
    "\n",
    "model = tf.keras.models.load_model('simpleRNN_attention.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6917e30c24a990",
   "metadata": {},
   "source": [
    "## Math Behind Attention Mechanism\n",
    "\n",
    "::::{note}\n",
    "The context vector is computed as follows:\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j\n",
    "$$\n",
    "\n",
    "where $c_i$ is the context vector at position $i$, $T_x$ is the length of the input sequence, $\\alpha_{ij}$ is the alignment score between the output at position $i$ and the input at position $j$, and $h_j$ is the hidden state at position $j$.\n",
    "::::\n",
    "\n",
    "![Attention Mechanism](https://raw.githubusercontent.com/ChaosTheLegend/ML-Book/main/Images/AttentionLive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac06f9478882cd1",
   "metadata": {},
   "source": [
    "| a_1  | a_2 | a_3 | a_4 | sum |\n",
    "|------|-----|-----|-----|-----|\n",
    "| 0.1  | 0.2 | 0.3 | 0.4 | 1.0 |\n",
    "| 0.5, | 0.6 | 0.7 | 0.8 | 2.6 |\n",
    "| 0.9  | 0.1 | 0.2 | 0.3 | 1.5 |\n",
    "| 0.4  | 0.3 | 0.6 | 0.7 | 2.0 |\n",
    "\n",
    "| h_1 | h_2 | h_3 | h_4 |\n",
    "|-----|-----|-----|-----|\n",
    "| 0.9 | 0.5 | 0.2 | 0.1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75106f95dc664398",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(git_path + 'quiz_scores.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e1885a299b644",
   "metadata": {},
   "source": [
    "## Alignment Scores\n",
    "\n",
    "::::{important}\n",
    "The alignment scores are computed using a score function, which measures how well the inputs around a given position and the output at that position match.\n",
    "::::\n",
    "\n",
    "There are many different score functions that can be used, but the most common one is the dot product:\n",
    "\n",
    "::::{note}\n",
    "$$\n",
    "score(h_i, h_j) = h_i^T h_j\n",
    "$$\n",
    "::::\n",
    "\n",
    "where $h_i$ is the hidden state at position $i$ and $h_j$ is the hidden state at position $j$.\n",
    "\n",
    "The alignment scores are then computed as follows:\n",
    "\n",
    "::::{note}\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{exp(score(h_i, h_j))}{\\sum_{k=1}^{T_x} exp(score(h_i, h_k))}\n",
    "$$\n",
    "::::\n",
    "\n",
    "where $T_x$ is the length of the input sequence.\n",
    "\n",
    "::::{note}\n",
    "The resulting matrix of alignment scores will look like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\alpha_{11} & \\alpha_{12} & \\alpha_{13} & \\dots & \\alpha_{1T_x} \\\\\n",
    "\\alpha_{21} & \\alpha_{22} & \\alpha_{23} & \\dots & \\alpha_{2T_x} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\alpha_{T_y1} & \\alpha_{T_y2} & \\alpha_{T_y3} & \\dots & \\alpha_{T_yT_x} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "::::\n",
    "\n",
    "In the beginning, the alignment scores are random, but as the model learns using backpropagation, the alignment scores are updated to reflect how well the inputs around a given position and the output at that position match.\n",
    "\n",
    "::::{note}\n",
    "$$\n",
    "new\\_alignment\\_scores = old\\_alignment\\_scores + \\frac{\\partial L}{\\partial \\alpha_{ij}}\n",
    "$$\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009ea0b",
   "metadata": {},
   "source": [
    "The following graph shows the sum of alignment scores after training the model for 5 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fecccee91bfd2e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "attention_layer = model.layers[2]\n",
    "\n",
    "weights = attention_layer.get_weights()[1]\n",
    "\n",
    "# sum all the columns in the weights matrix\n",
    "\n",
    "weights = np.sum(weights, axis=1)\n",
    "\n",
    "weights = weights[:20]\n",
    "\n",
    "plt.bar(range(weights.shape[0]), weights)\n",
    "\n",
    "plt.title('Attention Weights')\n",
    "\n",
    "plt.xlabel('Embedded word index')\n",
    "\n",
    "plt.ylabel('Word Weight')\n",
    "\n",
    "# make x axis use whole numbers\n",
    "\n",
    "plt.xticks(range(weights.shape[0]), range(weights.shape[0]))\n",
    "\n",
    "# color all negative weight bars red and positive weights blue\n",
    "\n",
    "for i in range(weights.shape[0]):\n",
    "    if weights[i] < 0:\n",
    "        plt.gca().get_children()[i].set_color('red')\n",
    "    else:\n",
    "        plt.gca().get_children()[i].set_color('green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e6bbcf75130aa5",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "The Lower the absolute value of the weight, the less relevant the word is to the output\n",
    "\n",
    "Green bars show positive weights, indicating that the word is positive\n",
    "\n",
    "Red bars show negative weights, indicating that the word is negative\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85609b1dc5e37d",
   "metadata": {},
   "source": [
    "The computed vector is then multiplied by the hidden state at position $i$ to produce the output at position $i$, this is being done in the context layer. In the case of Binary Classification, we use dense layer with sigmoid activation function to produce the output.\n",
    "\n",
    "::::{note}\n",
    "$$\n",
    "sum =  \\sum_{i=1}^{T_x} \\alpha_{ij} h_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "output = \\sigma(sum)\n",
    "$$\n",
    "::::\n",
    "\n",
    "But attention can also be used for other tasks such as Machine Translation, where another RNN is used to decode the output sequence.\n",
    "(See Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba7974def32f75",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(git_path + 'quiz_att.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d2e06e5ce9220",
   "metadata": {},
   "source": [
    "## Using Attention Mechanism\n",
    "\n",
    "By adding an attention mechanism, our model performs way better even when using the low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e324f3877247619",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# draw a bar chart to compare the accuracy of the two models\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "attention_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy = [attention_accuracy, simple_accuracy]\n",
    "\n",
    "\n",
    "plt.bar(['Attention (5 epochs)', 'Simple RNN (50 epochs)'], accuracy)\n",
    "\n",
    "# add a title to the plot\n",
    "\n",
    "plt.title('Accuracy of Simple RNN vs Attention Mechanism')\n",
    "\n",
    "# add a label to the y-axis\n",
    "\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7334cdb5d902a3",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "\n",
    "Attention Mechanism can also be used with other RNN implementations such as LSTM and GRU.\n",
    "\n",
    "It can also be used with Bidirectional RNNs, and even with CNNs.\n",
    "\n",
    "Here's an example of using Attention Mechanism with Bidirectional LSTM:\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eda4f7dddb9ff6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_len,))\n",
    "embedding = Embedding(input_dim=10000, output_dim=embedding_dim, input_length=max_len)(inputs)\n",
    "lstm = Bidirectional(LSTM(hidden_dim, return_sequences=True))(embedding)\n",
    "attention = Attention()([lstm, lstm])\n",
    "context = tf.reduce_sum(attention * lstm, axis=1)\n",
    "dropout = Dropout(dropout_rate)(context)\n",
    "output = Dense(output_dim, activation='sigmoid')(dropout)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "LSTM = tf.keras.models.load_model('LSTM.keras')\n",
    "\n",
    "y_pred = LSTM.predict(x_test)\n",
    "\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "lstm_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy = [attention_accuracy, lstm_accuracy, simple_accuracy]\n",
    "\n",
    "plt.bar(['Attention (5 epochs)', 'LSTM (5 epochs)', 'Simple RNN (50 epochs)'], accuracy)\n",
    "\n",
    "plt.title('Accuracy of Different RNNs')\n",
    "\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f130781b59dc8899",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "::::{important}\n",
    "Attention Mechanism is a way to help RNNs learn long-term dependencies by allowing the model to focus on the most relevant parts of the input sequence when producing a given output.\n",
    "::::\n",
    "\n",
    "::::{important}\n",
    "It does this by adding a context vector to the model, which is a weighted sum of the encoder's hidden states. The weights are computed using an alignment score function, which measures how well the inputs around a given position and the output at that position match.\n",
    "::::"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
